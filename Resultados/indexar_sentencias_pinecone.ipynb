{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb3b686",
   "metadata": {},
   "source": [
    "# Indexación de sentencias judiciales en Pinecone\n",
    "\n",
    "## Objetivo\n",
    "Cargar las sentencias del archivo Excel en `../Datos/sentencias_pasadas.xlsx`, generar embeddings con **OpenAI** e indexar los vectores en **Pinecone** para búsqueda semántica.\n",
    "\n",
    "## Requisitos previos\n",
    "- **OpenAI API Key**: para el modelo de embeddings (`text-embedding-3-small`).\n",
    "- **Pinecone API Key**: para crear/usar un índice en [Pinecone](https://app.pinecone.io/).\n",
    "\n",
    "## Configuración de claves (seguridad)\n",
    "Las claves **no** se escriben en el código. Se leen desde variables de entorno:\n",
    "1. Crea un archivo `.env` en la carpeta **Resultados** (junto a este notebook).\n",
    "2. Añade las líneas (sustituye por tus claves reales):\n",
    "   ```\n",
    "   OPENAI_API_KEY=sk-...\n",
    "   PINECONE_API_KEY=...\n",
    "   ```\n",
    "3. El archivo `.env` no debe subirse a control de versiones (ya está en `.gitignore` si usas Git).\n",
    "\n",
    "Puedes copiar `.env.example` a `.env` y rellenar los valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec, Metric\n",
    "\n",
    "# Cargar variables desde .env (en la carpeta Resultados, junto a este notebook)\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"Falta OPENAI_API_KEY. Configúrala en el archivo .env (ver celdas de documentación).\"\n",
    "    )\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"Falta PINECONE_API_KEY. Configúrala en el archivo .env (ver celdas de documentación).\"\n",
    "    )\n",
    "\n",
    "print(\"Variables de entorno cargadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Carga y preparación de datos\n",
    "\n",
    "Se lee el Excel de sentencias. Las columnas pueden tener valores faltantes; se construye un texto único para embedding a partir de **sintesis**, **Resuelve** y **Tema - subtema** (los que existan y tengan contenido). El resto de columnas se conservan como metadatos para filtrado en Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al Excel: desde Resultados/ se sube a ../Datos/ (ejecutar el notebook desde la carpeta Resultados)\n",
    "PATH_EXCEL = Path(\"../Datos/sentencias_pasadas.xlsx\")\n",
    "if not PATH_EXCEL.exists():\n",
    "    PATH_EXCEL = Path(\"Datos/sentencias_pasadas.xlsx\")  # por si se ejecuta desde la raíz del proyecto\n",
    "if not PATH_EXCEL.exists():\n",
    "    raise FileNotFoundError(f\"No se encontró el archivo: {PATH_EXCEL}\")\n",
    "\n",
    "df = pd.read_excel(PATH_EXCEL, engine=\"openpyxl\")\n",
    "\n",
    "# Eliminar columnas completamente vacías para simplificar\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "print(\"Columnas disponibles:\", list(df.columns))\n",
    "print(f\"\\nRegistros cargados: {len(df)}\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas usadas para construir el texto a embedir (prioridad: sintesis > Resuelve > Tema)\n",
    "COLUMNAS_TEXTO = [\"sintesis\", \"Resuelve\", \"Tema - subtema\"]\n",
    "\n",
    "def construir_texto_embedding(fila: pd.Series) -> str:\n",
    "    \"\"\"Concatena en un solo texto las columnas con contenido (evita NaN y cadenas vacías).\"\"\"\n",
    "    partes = []\n",
    "    for col in COLUMNAS_TEXTO:\n",
    "        if col in fila.index:\n",
    "            val = fila[col]\n",
    "            if pd.notna(val) and str(val).strip():\n",
    "                partes.append(str(val).strip())\n",
    "    return \" \\n \".join(partes) if partes else \"\"\n",
    "\n",
    "df[\"texto_embedding\"] = df.apply(construir_texto_embedding, axis=1)\n",
    "\n",
    "# Filtrar filas sin texto para embedir\n",
    "df_valid = df[df[\"texto_embedding\"].str.len() > 0].copy()\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "print(f\"Registros con texto para embedding: {len(df_valid)} de {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Embeddings con OpenAI\n",
    "\n",
    "Se usa el modelo `text-embedding-3-small` (dimensión 1536). Las peticiones se envían por lotes para no superar límites de la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "DIMENSION = 1536\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "client_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def obtener_embeddings(textos: list[str]) -> list[list[float]]:\n",
    "    \"\"\"Obtiene embeddings por lotes usando la API de OpenAI.\"\"\"\n",
    "    resultados = []\n",
    "    for i in range(0, len(textos), BATCH_SIZE):\n",
    "        lote = textos[i : i + BATCH_SIZE]\n",
    "        # La API no acepta cadenas vacías\n",
    "        lote = [t if t and t.strip() else \" \" for t in lote]\n",
    "        resp = client_openai.embeddings.create(model=EMBEDDING_MODEL, input=lote)\n",
    "        orden = sorted(resp.data, key=lambda x: x.index)\n",
    "        resultados.extend([e.embedding for e in orden])\n",
    "    return resultados\n",
    "\n",
    "textos = df_valid[\"texto_embedding\"].astype(str).tolist()\n",
    "print(f\"Generando embeddings para {len(textos)} textos (modelo: {EMBEDDING_MODEL})...\")\n",
    "embeddings = obtener_embeddings(textos)\n",
    "print(f\"Obtenidos {len(embeddings)} vectores de dimensión {len(embeddings[0]) if embeddings else 0}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Índice Pinecone\n",
    "\n",
    "Se crea un índice serverless (si no existe) con dimensión 1536 y métrica coseno. Luego se hace **upsert** de los vectores con metadatos (tipo, fecha, tema, etc.) para poder filtrar en búsquedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"sentencias-judiciales\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "if INDEX_NAME not in [idx.name for idx in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=DIMENSION,\n",
    "        metric=Metric.COSINE,\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=REGION),\n",
    "    )\n",
    "    print(f\"Índice '{INDEX_NAME}' creado.\")\n",
    "else:\n",
    "    print(f\"Índice '{INDEX_NAME}' ya existe.\")\n",
    "\n",
    "index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar metadatos para Pinecone (solo tipos JSON-serializables: str, int, float, bool)\n",
    "import numpy as np\n",
    "\n",
    "def _a_nativo(val):\n",
    "    \"\"\"Convierte numpy/pandas a tipos nativos de Python para que Pinecone pueda serializar.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    if isinstance(val, (np.integer, np.int64, np.int32)):\n",
    "        return int(val)\n",
    "    if isinstance(val, (np.floating, np.float64, np.float32)):\n",
    "        return float(val)\n",
    "    if isinstance(val, np.bool_):\n",
    "        return bool(val)\n",
    "    if isinstance(val, str):\n",
    "        return val[:40_000] if len(val) > 40_000 else val  # Límite Pinecone por valor\n",
    "    if isinstance(val, (int, float, bool)):\n",
    "        return val\n",
    "    return str(val)\n",
    "\n",
    "def fila_a_metadata(fila: pd.Series) -> dict:\n",
    "    meta = {}\n",
    "    for col in df_valid.columns:\n",
    "        if col == \"texto_embedding\":\n",
    "            continue\n",
    "        val = fila[col]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        v = _a_nativo(val)\n",
    "        if v is not None:\n",
    "            meta[col] = v\n",
    "    return meta\n",
    "\n",
    "UPSERT_BATCH = 100\n",
    "ids = [f\"sentencia_{i}\" for i in range(len(df_valid))]\n",
    "\n",
    "for i in range(0, len(ids), UPSERT_BATCH):\n",
    "    batch_ids = ids[i : i + UPSERT_BATCH]\n",
    "    batch_vectors = embeddings[i : i + UPSERT_BATCH]\n",
    "    batch_meta = [fila_a_metadata(df_valid.iloc[j]) for j in range(i, min(i + UPSERT_BATCH, len(df_valid)))]\n",
    "    vectors = [\n",
    "        {\"id\": id_, \"values\": vec, \"metadata\": meta}\n",
    "        for id_, vec, meta in zip(batch_ids, batch_vectors, batch_meta)\n",
    "    ]\n",
    "    index.upsert(vectors=vectors)\n",
    "    print(f\"Upsert {i + len(batch_ids)} / {len(ids)}\")\n",
    "\n",
    "print(\"Indexación en Pinecone completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comprobación\n",
    "\n",
    "Consulta de estadísticas del índice y una búsqueda por similitud de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = index.describe_index_stats()\n",
    "print(\"Estadísticas del índice:\", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: buscar sentencias similares a una consulta en lenguaje natural\n",
    "consulta = \"indemnización por despido\"\n",
    "resp_embed = client_openai.embeddings.create(model=EMBEDDING_MODEL, input=[consulta])\n",
    "vector_consulta = resp_embed.data[0].embedding\n",
    "\n",
    "resultados = index.query(vector=vector_consulta, top_k=3, include_metadata=True)\n",
    "print(f\"Top 3 resultados para: '{consulta}'\")\n",
    "for m in resultados.matches:\n",
    "    meta = m.metadata or {}\n",
    "    sintesis = meta.get(\"sintesis\", \"\")[:200]\n",
    "    print(f\"  - id: {m.id} | score: {m.score:.4f} | sintesis: {sintesis}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
